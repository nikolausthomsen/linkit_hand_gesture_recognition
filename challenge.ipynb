{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Linkit Challenge WS 2022/23 Hand Gesture Detection - Team X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup\n",
    "`pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Annotation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1. Import Packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philippkiesling/.conda/envs/linkit_hand_gesture_recognition/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl # PyTorch Lightning for easier training and evaluation of models\n",
    "import torch # PyTorch\n",
    "import cv2 as cv2 # OpenCV for image processing\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib.patches as patches  # for plotting bounding boxes\n",
    "%matplotlib inline\n",
    "import uuid   # Unique identifier\n",
    "import os # File system operations\n",
    "import time # Time operations\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2. Load and Test Pretrained Model\n",
    "In Computer Vision, we usually use pretrained Models, to reduce the number of samples required for training. In this case, we use a pretrained YOLOv5 model, which was trained on the COCO dataset. The COCO dataset contains 80 different classes, which are not relevant for our task. For now, let's just test the model on a random image.\n",
    "\n",
    "#### Tasks:\n",
    "_**Task 3.1:**_ Can you find better models for our task?\n",
    "_**Task 3.2:**_ Can you find better pretrained weights for our task?\n",
    "_Note:_ You can find possible models and weights on:\n",
    "1. huggingface [Model Hub](https://huggingface.co/models?pipeline_tag=object-detection&sort=downloads).\n",
    "2. pytorch [Model Zoo](https://pytorch.org/docs/stable/torchvision/models.html).\n",
    "3. pytorch [Model Hub](https://pytorch.org/hub/).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/philippkiesling/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2022-12-5 Python-3.10.8 torch-1.13.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We Load  an Image and run the model on it. The model returns a list of bounding boxes, with their corresponding class and confidence score."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv2.imread('datasets/example/zidane.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "results = model(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Display image and Convert to RGB, display labels and bounding boxes from the results with cv2\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img[:,:,::-1])\n",
    "\n",
    "# Draw bounding boxes and labels of detections\n",
    "for *rect,  conf, name, cls   in results.pandas().xyxy[0].values:\n",
    "    x1, y1, x2, y2 = rect\n",
    "    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1, f'{cls} {conf:.2f}', fontsize=12, c='white')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Adapt the Model for our Task\n",
    "We need to adapt the model for our task. Therefore, we need to remove the last layer of the model, and replace it with a new layer, which only contains 3 classes (one for each hand gesture).\n",
    "Since we use the [yolov5 implementation of ultralytics](https://github.com/ultralytics/yolov5), we can use their provided training script to train our model.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we define our 3 label names, and the path to the images and labels.\n",
    "\n",
    "This is done in the dataset.yaml file.\n",
    "Here we have to define the 3 classes, and the path to the images and labels.\n",
    "Make sure to use the correct mapping between classID and label\n",
    "1. 0 -> \"rock\"\n",
    "2. 1 -> \"paper\"\n",
    "3. 2 -> \"scissors\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we train our model.\n",
    "we use the dataset.yml file to define the path to the images and labels, and the number of classes.\n",
    "\n",
    "For different Parameter Configurations refer to:\n",
    "https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mphilippkiesling\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mweights=yolov5s.pt, cfg=, data=../dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=0, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\n",
      "\u001B[34m\u001B[1mgithub: \u001B[0m‚ö†Ô∏è YOLOv5 is out of date by 1 commit. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\r\n",
      "YOLOv5 üöÄ 2022-12-5 Python-3.10.8 torch-1.13.0 CPU\r\n",
      "\r\n",
      "\u001B[34m\u001B[1mhyperparameters: \u001B[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\n",
      "\u001B[34m\u001B[1mClearML: \u001B[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\r\n",
      "\u001B[34m\u001B[1mComet: \u001B[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\r\n",
      "\u001B[34m\u001B[1mTensorBoard: \u001B[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Tracking run with wandb version 0.13.5\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run data is saved locally in \u001B[35m\u001B[1m/Users/philippkiesling/PycharmProjects/linkit_hand_gesture_recognition/yolov5/wandb/run-20221206_014307-1beaf9z9\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run \u001B[1m`wandb offline`\u001B[0m to turn off syncing.\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Syncing run \u001B[33mprime-brook-3\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: ‚≠êÔ∏è View project at \u001B[34m\u001B[4mhttps://wandb.ai/philippkiesling/YOLOv5\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: üöÄ View run at \u001B[34m\u001B[4mhttps://wandb.ai/philippkiesling/YOLOv5/runs/1beaf9z9\u001B[0m\r\n",
      "\r\n",
      "                 from  n    params  module                                  arguments                     \r\n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \r\n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \r\n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \r\n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \r\n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \r\n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \r\n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \r\n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \r\n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \r\n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \r\n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \r\n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \r\n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \r\n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \r\n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \r\n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \r\n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \r\n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \r\n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \r\n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \r\n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \r\n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \r\n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\r\n",
      "Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\r\n",
      "\r\n",
      "Transferred 349/349 items from yolov5s.pt\r\n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning /Users/philippkiesling/PycharmProjects/linkit_hand_gesture_recog\u001B[0m\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mCaching images (0.1GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 1637.6\u001B[0m\r\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning /Users/philippkiesling/PycharmProjects/linkit_hand_gesture_recogni\u001B[0m\r\n",
      "\u001B[34m\u001B[1mval: \u001B[0mCaching images (0.1GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 2021.84i\u001B[0m\r\n",
      "\r\n",
      "\u001B[34m\u001B[1mAutoAnchor: \u001B[0m4.27 anchors/target, 0.994 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\r\n",
      "Plotting labels to runs/train/exp14/labels.jpg... \r\n",
      "Image sizes 640 train, 640 val\r\n",
      "Using 0 dataloader workers\r\n",
      "Logging results to \u001B[1mruns/train/exp14\u001B[0m\r\n",
      "Starting training for 3 epochs...\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\r\n",
      "        0/2         0G    0.04387    0.06956    0.01738        187        640:  "
     ]
    }
   ],
   "source": [
    "# Train YOLOv5s on COCO128 for 3 epochs\n",
    "!cd yolov5 && python train.py --img 640 --batch 16 --epochs 3 --data ../dataset.yaml --weights yolov5s.pt --cache --workers 0 --save-period 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}