{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Linkit Challenge WS 2022/23 Hand Gesture Detection - Team X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup\n",
    "`pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Annotation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1. Import Packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philippkiesling/.conda/envs/linkit_hand_gesture_recognition/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl # PyTorch Lightning for easier training and evaluation of models\n",
    "import torch # PyTorch\n",
    "import cv2 as cv2 # OpenCV for image processing\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib.patches as patches  # for plotting bounding boxes\n",
    "%matplotlib inline\n",
    "import uuid   # Unique identifier\n",
    "import os # File system operations\n",
    "import time # Time operations\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2. Load and Test Pretrained Model\n",
    "In Computer Vision, we usually use pretrained Models, to reduce the number of samples required for training. In this case, we use a pretrained YOLOv5 model, which was trained on the COCO dataset. The COCO dataset contains 80 different classes, which are not relevant for our task. For now, let's just test the model on a random image.\n",
    "\n",
    "#### Tasks:\n",
    "_**Task 3.1:**_ Can you find better models for our task?\n",
    "_**Task 3.2:**_ Can you find better pretrained weights for our task?\n",
    "_Note:_ You can find possible models and weights on:\n",
    "1. huggingface [Model Hub](https://huggingface.co/models?pipeline_tag=object-detection&sort=downloads).\n",
    "2. pytorch [Model Zoo](https://pytorch.org/docs/stable/torchvision/models.html).\n",
    "3. pytorch [Model Hub](https://pytorch.org/hub/).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/philippkiesling/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2022-12-5 Python-3.10.8 torch-1.13.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We Load  an Image and run the model on it. The model returns a list of bounding boxes, with their corresponding class and confidence score."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv2.imread('datasets/example/zidane.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "results = model(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Display image and Convert to RGB, display labels and bounding boxes from the results with cv2\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img[:,:,::-1])\n",
    "\n",
    "# Draw bounding boxes and labels of detections\n",
    "for *rect,  conf, name, cls   in results.pandas().xyxy[0].values:\n",
    "    x1, y1, x2, y2 = rect\n",
    "    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1, f'{cls} {conf:.2f}', fontsize=12, c='white')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Adapt the Model for our Task\n",
    "We need to adapt the model for our task. Therefore, we need to remove the last layer of the model, and replace it with a new layer, which only contains 3 classes (one for each hand gesture).\n",
    "Since we use the [yolov5 implementation of ultralytics](https://github.com/ultralytics/yolov5), we can use their provided training script to train our model.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we define our 3 label names, and the path to the images and labels.\n",
    "\n",
    "This is done in the dataset.yaml file.\n",
    "Here we have to define the 3 classes, and the path to the images and labels.\n",
    "Make sure to use the correct mapping between classID and label\n",
    "1. 0 -> \"rock\"\n",
    "2. 1 -> \"paper\"\n",
    "3. 2 -> \"scissors\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we train our model.\n",
    "we use the dataset.yml file to define the path to the images and labels, and the number of classes.\n",
    "\n",
    "For different Parameter Configurations refer to:\n",
    "https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data\n",
    "\n",
    "### Monitoring\n",
    "It is important to monitor the training process to ensure that the model is training properly.\n",
    "To do so, we recommend [Weights and Biases](https://wandb.ai/) (or [tensorboard](https://www.tensorflow.org/tensorboard)).\n",
    "Both tools keep track of the training process and automatically log the results.\n",
    "#### Weights and Biases\n",
    "1. Create an account on [Weights and Biases](https://wandb.ai/)\n",
    "2. Install the wandb package `pip install wandb`\n",
    "3. Login to your account `wandb login`\n",
    "4. Run the training script with the `--project` flag `python train.py --project <project_name>`\n",
    "5. Go to your [Weights and Biases](https://wandb.ai/) dashboard to view the results\n",
    "\n",
    "For more information on the YOLOV5 integration with Weights and Biases, refer to [here](https://docs.wandb.ai/guides/integrations/yolov5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mphilippkiesling\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mweights=yolov5s.pt, cfg=, data=../dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=0, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\n",
      "\u001B[34m\u001B[1mgithub: \u001B[0m‚ö†Ô∏è YOLOv5 is out of date by 1 commit. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\r\n",
      "YOLOv5 üöÄ 2022-12-5 Python-3.10.8 torch-1.13.0 CPU\r\n",
      "\r\n",
      "\u001B[34m\u001B[1mhyperparameters: \u001B[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\n",
      "\u001B[34m\u001B[1mClearML: \u001B[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\r\n",
      "\u001B[34m\u001B[1mComet: \u001B[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\r\n",
      "\u001B[34m\u001B[1mTensorBoard: \u001B[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Tracking run with wandb version 0.13.5\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run data is saved locally in \u001B[35m\u001B[1m/Users/philippkiesling/PycharmProjects/linkit_hand_gesture_recognition/yolov5/wandb/run-20221206_014307-1beaf9z9\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run \u001B[1m`wandb offline`\u001B[0m to turn off syncing.\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Syncing run \u001B[33mprime-brook-3\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: ‚≠êÔ∏è View project at \u001B[34m\u001B[4mhttps://wandb.ai/philippkiesling/YOLOv5\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: üöÄ View run at \u001B[34m\u001B[4mhttps://wandb.ai/philippkiesling/YOLOv5/runs/1beaf9z9\u001B[0m\r\n",
      "\r\n",
      "                 from  n    params  module                                  arguments                     \r\n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \r\n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \r\n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \r\n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \r\n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \r\n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \r\n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \r\n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \r\n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \r\n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \r\n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \r\n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \r\n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \r\n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \r\n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \r\n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \r\n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \r\n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \r\n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \r\n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \r\n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \r\n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \r\n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\r\n",
      "Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\r\n",
      "\r\n",
      "Transferred 349/349 items from yolov5s.pt\r\n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning /Users/philippkiesling/PycharmProjects/linkit_hand_gesture_recog\u001B[0m\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mCaching images (0.1GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 1637.6\u001B[0m\r\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning /Users/philippkiesling/PycharmProjects/linkit_hand_gesture_recogni\u001B[0m\r\n",
      "\u001B[34m\u001B[1mval: \u001B[0mCaching images (0.1GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 2021.84i\u001B[0m\r\n",
      "\r\n",
      "\u001B[34m\u001B[1mAutoAnchor: \u001B[0m4.27 anchors/target, 0.994 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\r\n",
      "Plotting labels to runs/train/exp14/labels.jpg... \r\n",
      "Image sizes 640 train, 640 val\r\n",
      "Using 0 dataloader workers\r\n",
      "Logging results to \u001B[1mruns/train/exp14\u001B[0m\r\n",
      "Starting training for 3 epochs...\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\r\n",
      "        0/2         0G    0.04507    0.06987    0.01726        207        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all        128        929      0.657      0.612      0.681      0.456\r\n",
      "Saving model artifact on epoch 1\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\r\n",
      "        1/2         0G    0.04501     0.0666    0.01598        335        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all        128        929      0.739      0.636      0.721       0.48\r\n",
      "Saving model artifact on epoch 2\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\r\n",
      "        2/2         0G    0.04412    0.06195    0.01664        156        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all        128        929      0.736      0.642       0.72      0.485\r\n",
      "\r\n",
      "3 epochs completed in 0.142 hours.\r\n",
      "Optimizer stripped from runs/train/exp14/weights/last.pt, 14.8MB\r\n",
      "Optimizer stripped from runs/train/exp14/weights/best.pt, 14.8MB\r\n",
      "\r\n",
      "Validating runs/train/exp14/weights/best.pt...\r\n",
      "Fusing layers... \r\n",
      "Model summary: 157 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all        128        929      0.738      0.642       0.72      0.488\r\n",
      "                person        128        254      0.856       0.72      0.805      0.523\r\n",
      "               bicycle        128          6      0.712      0.423      0.623      0.461\r\n",
      "                   car        128         46      0.829      0.348      0.542       0.24\r\n",
      "            motorcycle        128          5      0.647        0.8      0.838      0.693\r\n",
      "              airplane        128          6      0.986          1      0.995      0.674\r\n",
      "                   bus        128          7      0.626      0.714      0.778      0.665\r\n",
      "                 train        128          3          1      0.586      0.913      0.586\r\n",
      "                 truck        128         12      0.707      0.417      0.502      0.294\r\n",
      "                  boat        128          6      0.819      0.333      0.578      0.239\r\n",
      "         traffic light        128         14      0.769      0.241      0.392      0.211\r\n",
      "             stop sign        128          2       0.79          1      0.995      0.821\r\n",
      "                 bench        128          9       0.83      0.545      0.628       0.27\r\n",
      "                  bird        128         16      0.965          1      0.995      0.615\r\n",
      "                   cat        128          4      0.753          1      0.995      0.797\r\n",
      "                   dog        128          9          1      0.657       0.89       0.66\r\n",
      "                 horse        128          2       0.81          1      0.995      0.622\r\n",
      "              elephant        128         17      0.919      0.882       0.93      0.705\r\n",
      "                  bear        128          1      0.686          1      0.995      0.995\r\n",
      "                 zebra        128          4      0.866          1      0.995      0.903\r\n",
      "               giraffe        128          9      0.788      0.831      0.901      0.715\r\n",
      "              backpack        128          6          1      0.651      0.787      0.325\r\n",
      "              umbrella        128         18      0.801      0.672      0.853      0.509\r\n",
      "               handbag        128         19      0.883      0.263      0.374       0.22\r\n",
      "                   tie        128          7      0.905      0.714      0.787      0.533\r\n",
      "              suitcase        128          4      0.619       0.75      0.912      0.596\r\n",
      "               frisbee        128          5      0.716        0.8      0.759      0.675\r\n",
      "                  skis        128          1          0          0     0.0622     0.0144\r\n",
      "             snowboard        128          7      0.716      0.571      0.811       0.54\r\n",
      "           sports ball        128          6       0.62      0.667      0.602      0.333\r\n",
      "                  kite        128         10      0.658        0.5      0.553      0.204\r\n",
      "          baseball bat        128          4      0.718        0.5      0.535      0.203\r\n",
      "        baseball glove        128          7      0.474      0.429      0.464      0.314\r\n",
      "            skateboard        128          5      0.897        0.8      0.854      0.521\r\n",
      "         tennis racket        128          7      0.708      0.429      0.533      0.308\r\n",
      "                bottle        128         18      0.651      0.416      0.558       0.27\r\n",
      "            wine glass        128         16       0.68      0.938      0.902      0.477\r\n",
      "                   cup        128         36      0.799      0.722      0.822      0.511\r\n",
      "                  fork        128          6      0.966      0.333      0.436      0.352\r\n",
      "                 knife        128         16      0.766      0.688      0.684      0.404\r\n",
      "                 spoon        128         22      0.838      0.472      0.645      0.364\r\n",
      "                  bowl        128         28      0.767      0.607      0.717      0.515\r\n",
      "                banana        128          1      0.908          1      0.995      0.399\r\n",
      "              sandwich        128          2          1          0       0.29      0.265\r\n",
      "                orange        128          4      0.761          1      0.995      0.629\r\n",
      "              broccoli        128         11      0.378      0.455      0.474      0.342\r\n",
      "                carrot        128         24      0.691      0.654      0.752      0.485\r\n",
      "               hot dog        128          2      0.417          1      0.828      0.745\r\n",
      "                 pizza        128          5      0.787          1      0.962      0.687\r\n",
      "                 donut        128         14      0.685          1       0.96      0.842\r\n",
      "                  cake        128          4      0.817          1      0.995      0.772\r\n",
      "                 chair        128         35      0.501        0.6      0.602      0.331\r\n",
      "                 couch        128          6      0.796      0.667      0.819      0.556\r\n",
      "          potted plant        128         14      0.708      0.786      0.841      0.498\r\n",
      "                   bed        128          3       0.49      0.333      0.731      0.427\r\n",
      "          dining table        128         13      0.823       0.36        0.6      0.362\r\n",
      "                toilet        128          2      0.744          1      0.995      0.846\r\n",
      "                    tv        128          2      0.597          1      0.995      0.796\r\n",
      "                laptop        128          3      0.957      0.333      0.571      0.343\r\n",
      "                 mouse        128          2          1          0      0.105     0.0526\r\n",
      "                remote        128          8      0.984      0.625      0.633       0.55\r\n",
      "            cell phone        128          8      0.662        0.5      0.382       0.21\r\n",
      "             microwave        128          3      0.715          1      0.995      0.766\r\n",
      "                  oven        128          5      0.325        0.4      0.439      0.289\r\n",
      "                  sink        128          6      0.429      0.333      0.349      0.247\r\n",
      "          refrigerator        128          5      0.547        0.8      0.798      0.543\r\n",
      "                  book        128         29      0.574      0.276      0.351      0.172\r\n",
      "                 clock        128          9      0.747      0.889      0.888      0.684\r\n",
      "                  vase        128          2      0.313          1      0.995      0.895\r\n",
      "              scissors        128          1          1          0      0.332     0.0332\r\n",
      "            teddy bear        128         21      0.814      0.524      0.785      0.519\r\n",
      "            toothbrush        128          5      0.699        0.6      0.762       0.46\r\n",
      "Results saved to \u001B[1mruns/train/exp14\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Waiting for W&B process to finish... \u001B[32m(success).\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run history:\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:      metrics/mAP_0.5 ‚ñÅ‚ñà‚ñà‚ñà\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: metrics/mAP_0.5:0.95 ‚ñÅ‚ñÜ‚ñá‚ñà\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    metrics/precision ‚ñÅ‚ñà‚ñà‚ñà\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:       metrics/recall ‚ñÅ‚ñá‚ñà‚ñà\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:       train/box_loss ‚ñà‚ñà‚ñÅ\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:       train/cls_loss ‚ñà‚ñÅ‚ñÖ\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:       train/obj_loss ‚ñà‚ñÖ‚ñÅ\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         val/box_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         val/cls_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         val/obj_loss ‚ñà‚ñÑ‚ñÅ‚ñÅ\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:                x/lr0 ‚ñà‚ñÖ‚ñÅ\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:                x/lr1 ‚ñÅ‚ñà‚ñÉ\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:                x/lr2 ‚ñÅ‚ñà‚ñÉ\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run summary:\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:           best/epoch 2\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         best/mAP_0.5 0.72045\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    best/mAP_0.5:0.95 0.48505\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:       best/precision 0.73606\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:          best/recall 0.64214\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:      metrics/mAP_0.5 0.72044\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: metrics/mAP_0.5:0.95 0.48766\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:    metrics/precision 0.73816\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:       metrics/recall 0.6416\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:       train/box_loss 0.04412\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:       train/cls_loss 0.01664\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:       train/obj_loss 0.06195\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         val/box_loss 0.04018\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         val/cls_loss 0.009\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         val/obj_loss 0.03675\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:                x/lr0 0.07778\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:                x/lr1 0.00078\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:                x/lr2 0.00078\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Synced \u001B[33mprime-brook-3\u001B[0m: \u001B[34m\u001B[4mhttps://wandb.ai/philippkiesling/YOLOv5/runs/1beaf9z9\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Synced 6 W&B file(s), 113 media file(s), 3 artifact file(s) and 0 other file(s)\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Find logs at: \u001B[35m\u001B[1m./wandb/run-20221206_014307-1beaf9z9/logs\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv5s on COCO128 for 3 epochs\n",
    "!cd yolov5 && python train.py --img 640 --batch 16 --epochs 3 --data ../dataset.yaml --weights yolov5s.pt --cache --workers 0 --save-period 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}