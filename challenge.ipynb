{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Linkit Challenge WS 2022/23 Hand Gesture Detection - Team X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup\n",
    "`pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Annotation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1. Import Packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philippkiesling/.conda/envs/linkit_hand_gesture_recognition/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl # PyTorch Lightning for easier training and evaluation of models\n",
    "import torch # PyTorch\n",
    "import cv2 as cv2 # OpenCV for image processing\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib.patches as patches  # for plotting bounding boxes\n",
    "%matplotlib inline\n",
    "import uuid   # Unique identifier\n",
    "import os # File system operations\n",
    "import time # Time operations\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2. Load and Test Pretrained Model\n",
    "In Computer Vision, we usually use pretrained Models, to reduce the number of samples required for training. In this case, we use a pretrained YOLOv5 model, which was trained on the COCO dataset. The COCO dataset contains 80 different classes, which are not relevant for our task. For now, let's just test the model on a random image.\n",
    "\n",
    "#### Tasks:\n",
    "_**Task 3.1:**_ Can you find better models for our task?\n",
    "_**Task 3.2:**_ Can you find better pretrained weights for our task?\n",
    "_Note:_ You can find possible models and weights on:\n",
    "1. huggingface [Model Hub](https://huggingface.co/models?pipeline_tag=object-detection&sort=downloads).\n",
    "2. pytorch [Model Zoo](https://pytorch.org/docs/stable/torchvision/models.html).\n",
    "3. pytorch [Model Hub](https://pytorch.org/hub/).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/philippkiesling/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2022-12-5 Python-3.10.8 torch-1.13.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We Load  an Image and run the model on it. The model returns a list of bounding boxes, with their corresponding class and confidence score."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv2.imread('datasets/example/zidane.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "results = model(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Display image and Convert to RGB, display labels and bounding boxes from the results with cv2\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img[:,:,::-1])\n",
    "\n",
    "# Draw bounding boxes and labels of detections\n",
    "for *rect,  conf, name, cls   in results.pandas().xyxy[0].values:\n",
    "    x1, y1, x2, y2 = rect\n",
    "    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1, f'{cls} {conf:.2f}', fontsize=12, c='white')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Adapt the Model for our Task\n",
    "We need to adapt the model for our task. Therefore, we need to remove the last layer of the model, and replace it with a new layer, which only contains 3 classes (one for each hand gesture).\n",
    "Since we use the [yolov5 implementation of ultralytics](https://github.com/ultralytics/yolov5), we can use their provided training script to train our model.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we define our 3 label names, and the path to the images and labels.\n",
    "\n",
    "This is done in the dataset.yaml file.\n",
    "Here we have to define the 3 classes, and the path to the images and labels.\n",
    "Make sure to use the correct mapping between classID and label\n",
    "1. 0 -> \"rock\"\n",
    "2. 1 -> \"paper\"\n",
    "3. 2 -> \"scissors\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we train our model.\n",
    "we use the dataset.yml file to define the path to the images and labels, and the number of classes.\n",
    "\n",
    "For different Parameter Configurations refer to:\n",
    "https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mphilippkiesling\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\r\n",
      "usage: train.py [-h] [--weights WEIGHTS] [--cfg CFG] [--data DATA] [--hyp HYP]\r\n",
      "                [--epochs EPOCHS] [--batch-size BATCH_SIZE] [--imgsz IMGSZ]\r\n",
      "                [--rect] [--resume [RESUME]] [--nosave] [--noval]\r\n",
      "                [--noautoanchor] [--noplots] [--evolve [EVOLVE]]\r\n",
      "                [--bucket BUCKET] [--cache [CACHE]] [--image-weights]\r\n",
      "                [--device DEVICE] [--multi-scale] [--single-cls]\r\n",
      "                [--optimizer {SGD,Adam,AdamW}] [--sync-bn] [--workers WORKERS]\r\n",
      "                [--project PROJECT] [--name NAME] [--exist-ok] [--quad]\r\n",
      "                [--cos-lr] [--label-smoothing LABEL_SMOOTHING]\r\n",
      "                [--patience PATIENCE] [--freeze FREEZE [FREEZE ...]]\r\n",
      "                [--save-period SAVE_PERIOD] [--seed SEED]\r\n",
      "                [--local_rank LOCAL_RANK] [--entity ENTITY]\r\n",
      "                [--upload_dataset [UPLOAD_DATASET]]\r\n",
      "                [--bbox_interval BBOX_INTERVAL]\r\n",
      "                [--artifact_alias ARTIFACT_ALIAS]\r\n",
      "train.py: error: unrecognized arguments: --save_period 1\r\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv5s on COCO128 for 3 epochs\n",
    "!cd yolov5 && python train.py --img 640 --batch 16 --epochs 3 --data ../dataset.yaml --weights yolov5s.pt --cache --workers 0 --save_period 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}